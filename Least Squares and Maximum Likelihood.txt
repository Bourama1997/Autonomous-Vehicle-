Bienvenue à la dernière leçon du Module 1. Nous terminerons le module en discutant d'une connexion importante que aidera à fournir plus d'intuition pour la méthode des moindres carrés. Plus précisément, à la fin de cette leçon, , vous serez en mesure d'indiquer la connexion entre la méthode des moindres carrés et l'estimation de probabilité maximale avec des variables aléatoires gaussiennes. Commençons par rappeler le critère des moindres carrés de la toute première vidéo de ce module. Nous avons trouvé les meilleures estimations de certains paramètres inconnus mais constants en déterminant les valeurs qui minimisent la somme des erreurs au carré en fonction de nos mesures. Mais on peut se demander : Pourquoi les erreurs au carré ? Pourquoi pas les erreurs en cubes, ou racine carrée, ou autre chose ? C'est en fait une question particulièrement profonde, et il y a tout un domaine de statistiques robustes qui lui sont dédiées. Vous pouvez en effet utiliser différentes fonctions d'erreur, mais nous reviendrons sur deux raisons pour lesquelles les erreurs au carré sont attrayantes et pertinentes. La première est simple. Les erreurs carrées nous permettent de résoudre pour les paramètres optimaux avec une algèbre relativement simple. Si le modèle de mesure est linéaire, minimiser le critère d'erreur carré revient à résoudre un système linéaire d'équations. La deuxième raison a à voir avec la probabilité et un lien profond entre moindres carrés et les estimateurs de probabilité maximale sous l'hypothèse du bruit gaussien. Comme vous l'avez peut-être deviné, cette connexion a d'abord été dérivée sous une forme particulière par Gauss, et il n'est donc pas surprenant qu'elle implique variables aléatoires gaussiennes ou équivalent, bruit gaussien. Pour comprendre ce lien fondamental, discutons d'abord de la probabilité maximale. Au lieu d'écrire une perte, nous pouvons aborder le problème de l'estimation optimale des paramètres en demandant quels paramètres font nos mesures enregistrées les plus probables. Pour garder les choses simples, nous nous en tiendrons à un seul paramètre scalaire pour construire notre intuition. Par exemple, regardons à nouveau la mesure de la résistance. Compte tenu de ce que nous savons sur la probabilité, si nous avons quatre valeurs possibles pour ce paramètre de résistance inconnu, X, x_a à x_d, et chacune donne lieu à la probabilité conditionnelle suivante sur notre mesure Y. Quelle valeur maximiserait le conditionnel probabilité étant donné la mesure y_meas ?. C'est exact, x_a. La densité de probabilité la plus élevée à l'emplacement mesuré est donnée par la courbe verte, ce qui signifie que x_a est notre valeur la plus probable de compte tenu de cette mesure. Maintenant, si nous prenons notre modèle de mesure simple, nous pouvons le convertir en densité de probabilité en en supposant une densité pour notre bruit additif. Le paramètre inconnu x devient la moyenne de cette densité, et la variance est simplement notre variance de bruit. Rappelons que la densité de probabilité de une variable aléatoire gaussienne est donnée par l'équation suivante. Cela signifie que nous pouvons exprimer notre probabilité de mesure pour une seule mesure comme suit. Si nous avons plusieurs mesures indépendantes, chacune avec la même variance de bruit, nous pouvons simplement prendre le produit de plusieurs Gaussiens qui donnent lieu à un autre Gaussien. Nous pouvons essayer de maximiser cette probabilité avec respect au paramètre moyen x hat. Pour ce faire, nous utiliserons une technique souvent utilisée dans l'optimisation. Au lieu de maximiser la probabilité directement, nous allons prendre son logarithme et maximiser cela. Puisque la probabilité sera toujours un nombre positif, et parce que le logarithme est une fonction monotone croissante, cela n'affectera pas le résultat final qui est très pratique. Une fois que nous appliquons le logarithme à cette probabilité, nous voyons que quelque chose surgit que ressemble beaucoup à la somme des erreurs carrées. La constante C dans cette expression fait référence à des termes qui ne sont pas une fonction de x et que nous pouvons ignorer en toute sécurité. La dernière étape est maintenant de réaliser que l'argmax de la fonction f est le même que l'argmin du négatif de cette fonction. En utilisant ce fait, nous pouvons transformer notre maximisation de probabilité en une minimisation de la somme des erreurs au carré. Voila, minimiser la somme des erreurs carrées équivaut à maximiser la probabilité d'un ensemble de mesures en supposant que les mesures sont corrompues par des additifs, Bruit gaussien indépendant qui est de variance égale. De plus, si nous maintenons les mêmes hypothèses, mais changeons la variance de le bruit gaussien pour chaque mesure, nous pouvons arriver au même critère que que nous avons vu dans notre vidéo des moindres carrés pondérés. Donc, dans les deux cas, l'estimation de probabilité maximale donnée additif bruit gaussien est équivalent aux solutions des moindres carrés ou des moindres carrés pondérés que nous avons dérivées plus tôt. Alors pourquoi ce résultat est-il si important ? Une voiture autonome devra faire face à beaucoup, de nombreuses sources d'erreur, dont certaines sont très difficiles à modéliser. Cependant, le théorème de la limite centrale nous dit qu'en combinant toutes ces erreurs ensemble, elles peuvent raisonnablement être modélisées par une seule distribution d'erreur gaussienne. Nous aimerions modéliser notre système de façon probabiliste tout en maintenant la simplicité des calculs. Si nos erreurs sont gaussiennes, alors la meilleure estimation de probabilité maximale des paramètres d'intérêt est exactement la solution des moindres carrés que nous connaissons déjà, facile. Pour finir, discutons une mise en garde importante pour cette méthode. Lorsque nous utilisons la méthode des moindres carrés, les valeurs aberrantes de mesure peuvent avoir un effet significatif sur notre estimation finale. Pour comprendre pourquoi, considère que dans une distribution gaussienne, un échantillon qui se situe à deux écarts-types de la moyenne a moins d'une probabilité de cinq pour cent de se produire. Par conséquent, s'il y a quelques valeurs aberrantes dans nos données de mesure, la méthode de la probabilité maximale et équivalente, la méthode des moindres carrés accordera une importance significative à ces mesures. Donc, la valeur estimée de notre paramètre sera fortement tirée par ces valeurs aberrantes. Notre méthode optimale sera biaisée de sorte que la mesure périphérique est plus probable. Cela peut arriver à de nombreux estimateurs utilisant des données de capteur provenant d'un véhicule autonome. Les valeurs aberrantes peuvent résulter de personnes qui marchent au milieu d'un scan Lidar, par exemple, ou d'un mauvais signal GPS. Considérez notre exemple de résistance. Avec une mesure périphérique qui peut provenir d'un simple accident, l'estimation finale est retirée significativement de celle du cas libre aberrant. Nous voulons toujours être conscients des valeurs aberrantes et essayer de quantifier la distribution des erreurs chaque fois que possible avant appliquer aveuglément la probabilité maximale ou les moindres carrés. Maintenant que nous avons établi ce lien entre la probabilité maximale et les moindres carrés, nous sommes prêts à étendre notre estimateur récursif des moindres carrés au filtre commun complet, l'un des algorithmes les plus célèbres du 20e siècle. Pour résumer, nous avons appris que la méthode des moindres carrés et des moindres carrés pondérés produit les mêmes estimations que la probabilité maximale de compte tenu du bruit gaussien. Ceci est particulièrement important parce que de nombreuses sources de bruit complexes quand ajoutée tendront vers un gaussien. Cependant, il est toujours important de se méfier des mesures aberrantes qui peuvent avoir une incidence significative sur nos valeurs d'estimation finales. Dans le module suivant, , nous allons jeter un coup d'oeil à comment nous pouvons maintenant étendre ce que nous avons appris sur les moindres carrés et l'estimation des paramètres à variant continuellement les états que peuvent avoir des modèles non linéaires plus complexes associés à eux. On se voit là-bas.